/**
 * Multimodal Tools - Ù‚Ø¯Ø±Ø§Øª Ø§Ù„ÙˆØ³Ø§Ø¦Ø· Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø©
 * ÙŠØ³Ù…Ø­ Ù„Ù€ JOE Ø¨ÙÙ‡Ù… ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØ± ÙˆØ§Ù„ØµÙˆØª ÙˆØ§Ù„ÙÙŠØ¯ÙŠÙˆ
 */

import OpenAI from 'openai';
import fs from 'fs/promises';
import axios from 'axios';
import path from 'path';

const openai = new OpenAI();

/**
 * ØªØ­Ù„ÙŠÙ„ ØµÙˆØ±Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Vision AI
 */
export async function analyzeImage(imageUrl, prompt = 'ØµÙ Ù‡Ø°Ù‡ Ø§Ù„ØµÙˆØ±Ø© Ø¨Ø§Ù„ØªÙØµÙŠÙ„') {
  try {
    console.log(`ğŸ‘ï¸ Analyzing image: ${imageUrl}`);

    const response = await openai.chat.completions.create({
      model: 'gpt-4o-mini',
      messages: [
        {
          role: 'user',
          content: [
            { type: 'text', text: prompt },
            {
              type: 'image_url',
              image_url: { url: imageUrl }
            }
          ]
        }
      ],
      max_tokens: 500
    });

    return {
      success: true,
      analysis: response.choices[0].message.content,
      imageUrl
    };
  } catch (error) {
    console.error('Analyze image error:', error.message);
    return {
      success: false,
      error: error.message
    };
  }
}

/**
 * ØªØ­Ù„ÙŠÙ„ ØµÙˆØ±Ø© Ù…Ø­Ù„ÙŠØ©
 */
export async function analyzeLocalImage(imagePath, prompt = 'ØµÙ Ù‡Ø°Ù‡ Ø§Ù„ØµÙˆØ±Ø© Ø¨Ø§Ù„ØªÙØµÙŠÙ„') {
  try {
    console.log(`ğŸ‘ï¸ Analyzing local image: ${imagePath}`);

    const imageBuffer = await fs.readFile(imagePath);
    const base64Image = imageBuffer.toString('base64');
    const ext = path.extname(imagePath).toLowerCase();
    
    let mimeType = 'image/jpeg';
    if (ext === '.png') mimeType = 'image/png';
    else if (ext === '.gif') mimeType = 'image/gif';
    else if (ext === '.webp') mimeType = 'image/webp';

    const dataUrl = `data:${mimeType};base64,${base64Image}`;

    const response = await openai.chat.completions.create({
      model: 'gpt-4o-mini',
      messages: [
        {
          role: 'user',
          content: [
            { type: 'text', text: prompt },
            {
              type: 'image_url',
              image_url: { url: dataUrl }
            }
          ]
        }
      ],
      max_tokens: 500
    });

    return {
      success: true,
      analysis: response.choices[0].message.content,
      imagePath
    };
  } catch (error) {
    console.error('Analyze local image error:', error.message);
    return {
      success: false,
      error: error.message
    };
  }
}

/**
 * Ø¥Ù†Ø´Ø§Ø¡ ØµÙˆØ±Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… DALL-E
 */
export async function generateImage(prompt, size = '1024x1024', quality = 'standard') {
  try {
    console.log(`ğŸ¨ Generating image: "${prompt}"`);

    const response = await openai.images.generate({
      model: 'dall-e-3',
      prompt,
      n: 1,
      size,
      quality
    });

    return {
      success: true,
      imageUrl: response.data[0].url,
      revisedPrompt: response.data[0].revised_prompt,
      prompt
    };
  } catch (error) {
    console.error('Generate image error:', error.message);
    return {
      success: false,
      error: error.message
    };
  }
}

/**
 * ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ Ù†Øµ
 */
export async function transcribeAudio(audioFilePath) {
  try {
    console.log(`ğŸ¤ Transcribing audio: ${audioFilePath}`);

    const audioFile = await fs.readFile(audioFilePath);
    const blob = new Blob([audioFile]);
    
    const response = await openai.audio.transcriptions.create({
      file: blob,
      model: 'whisper-1',
      language: 'ar'
    });

    return {
      success: true,
      text: response.text,
      audioFilePath
    };
  } catch (error) {
    console.error('Transcribe audio error:', error.message);
    return {
      success: false,
      error: error.message
    };
  }
}

/**
 * ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ØµÙˆØª
 */
export async function textToSpeech(text, voice = 'alloy', outputPath = null) {
  try {
    console.log(`ğŸ”Š Converting text to speech...`);

    const response = await openai.audio.speech.create({
      model: 'tts-1',
      voice, // alloy, echo, fable, onyx, nova, shimmer
      input: text
    });

    const buffer = Buffer.from(await response.arrayBuffer());

    if (!outputPath) {
      outputPath = path.join(process.cwd(), 'temp', `speech_${Date.now()}.mp3`);
      await fs.mkdir(path.dirname(outputPath), { recursive: true });
    }

    await fs.writeFile(outputPath, buffer);

    return {
      success: true,
      audioPath: outputPath,
      text
    };
  } catch (error) {
    console.error('Text to speech error:', error.message);
    return {
      success: false,
      error: error.message
    };
  }
}

/**
 * Ù…Ù‚Ø§Ø±Ù†Ø© ØµÙˆØ±ØªÙŠÙ†
 */
export async function compareImages(imageUrl1, imageUrl2) {
  try {
    console.log(`ğŸ” Comparing two images...`);

    const response = await openai.chat.completions.create({
      model: 'gpt-4o-mini',
      messages: [
        {
          role: 'user',
          content: [
            { type: 'text', text: 'Ù‚Ø§Ø±Ù† Ø¨ÙŠÙ† Ù‡Ø§ØªÙŠÙ† Ø§Ù„ØµÙˆØ±ØªÙŠÙ† ÙˆØ§Ø°ÙƒØ± Ø£ÙˆØ¬Ù‡ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ ÙˆØ§Ù„Ø§Ø®ØªÙ„Ø§Ù' },
            { type: 'image_url', image_url: { url: imageUrl1 } },
            { type: 'image_url', image_url: { url: imageUrl2 } }
          ]
        }
      ],
      max_tokens: 500
    });

    return {
      success: true,
      comparison: response.choices[0].message.content,
      images: [imageUrl1, imageUrl2]
    };
  } catch (error) {
    console.error('Compare images error:', error.message);
    return {
      success: false,
      error: error.message
    };
  }
}

/**
 * Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ù…Ù† ØµÙˆØ±Ø© (OCR)
 */
export async function extractTextFromImage(imageUrl) {
  try {
    console.log(`ğŸ“ Extracting text from image...`);

    const response = await openai.chat.completions.create({
      model: 'gpt-4o-mini',
      messages: [
        {
          role: 'user',
          content: [
            { type: 'text', text: 'Ø§Ø³ØªØ®Ø±Ø¬ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„ØµÙˆØ±Ø©' },
            { type: 'image_url', image_url: { url: imageUrl } }
          ]
        }
      ],
      max_tokens: 500
    });

    return {
      success: true,
      extractedText: response.choices[0].message.content,
      imageUrl
    };
  } catch (error) {
    console.error('Extract text from image error:', error.message);
    return {
      success: false,
      error: error.message
    };
  }
}

/**
 * ØªØ­Ù„ÙŠÙ„ Ù…Ø­ØªÙˆÙ‰ ÙÙŠØ¯ÙŠÙˆ (Ø¹Ø¨Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª)
 */
export async function analyzeVideo(videoPath, frameInterval = 5) {
  try {
    console.log(`ğŸ¬ Analyzing video: ${videoPath}`);

    // Ù…Ù„Ø§Ø­Ø¸Ø©: Ù‡Ø°Ù‡ Ø¯Ø§Ù„Ø© Ù…Ø¨Ø³Ø·Ø©ØŒ ÙÙŠ Ø§Ù„ÙˆØ§Ù‚Ø¹ ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ù…ÙƒØªØ¨Ø© Ù…Ø«Ù„ ffmpeg
    // Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª Ù…Ù† Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø«Ù… ØªØ­Ù„ÙŠÙ„Ù‡Ø§

    return {
      success: true,
      message: 'ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ ÙŠØªØ·Ù„Ø¨ Ù…ÙƒØªØ¨Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ© (ffmpeg)',
      videoPath
    };
  } catch (error) {
    console.error('Analyze video error:', error.message);
    return {
      success: false,
      error: error.message
    };
  }
}

/**
 * ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ø§Ù„ØµÙˆØ±Ø©
 */
export async function detectObjects(imageUrl) {
  try {
    console.log(`ğŸ” Detecting objects in image...`);

    const response = await openai.chat.completions.create({
      model: 'gpt-4o-mini',
      messages: [
        {
          role: 'user',
          content: [
            { type: 'text', text: 'Ø­Ø¯Ø¯ Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„ØµÙˆØ±Ø© ÙˆØ§Ø°ÙƒØ± Ù…ÙˆÙ‚Ø¹Ù‡Ø§' },
            { type: 'image_url', image_url: { url: imageUrl } }
          ]
        }
      ],
      max_tokens: 500
    });

    return {
      success: true,
      objects: response.choices[0].message.content,
      imageUrl
    };
  } catch (error) {
    console.error('Detect objects error:', error.message);
    return {
      success: false,
      error: error.message
    };
  }
}

export const multimodalTools = {
  analyzeImage,
  analyzeLocalImage,
  generateImage,
  transcribeAudio,
  textToSpeech,
  compareImages,
  extractTextFromImage,
  analyzeVideo,
  detectObjects
};
